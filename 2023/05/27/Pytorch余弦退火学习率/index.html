<!DOCTYPE html><html lang="en" theme-mode="dark"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Pytorch余弦退火学习率 | 热带海藻的博客</title><link rel="icon" type="image/x-icon" href="/favicon.ico"><script>var config = {"root":"/","search":{"preload":true,"activeHolder":"Enter here","blurHolder":"Search","noResult":"Data \"$0\" not found"},"code":{"codeInfo":"$0 - $1 lines","copy":"copy","copyFinish":"copied","expand":"expand"}}</script><script src="//unpkg.com/valine/dist/Valine.min.js"></script><script src="//unpkg.com/mermaid@9.2.2/dist/mermaid.min.js"></script><link type="text/css" rel="stylesheet" href="//unpkg.com/lightgallery@2.7.1/css/lightgallery.css"><link type="text/css" rel="stylesheet" href="//unpkg.com/lightgallery@2.7.1/css/lg-zoom.css"><link type="text/css" rel="stylesheet" href="//unpkg.com/lightgallery@2.7.1/css/lg-thumbnail.css"><link type="text/css" rel="stylesheet" href="/lib/fontawesome/brands.min.css"><link type="text/css" rel="stylesheet" href="/lib/fontawesome/fontawesome.min.css"><link rel="stylesheet" href="/css/arknights.css"><script>if (window.localStorage.getItem('theme-mode') === 'light') document.documentElement.setAttribute('theme-mode', 'light')
if (window.localStorage.getItem('theme-mode') === 'dark') document.documentElement.setAttribute('theme-mode', 'dark')</script><style>@font-face {
 font-family: BenderLight;
 src: local('Bender'), url("/font/BenderLight.ttf");
}
@font-face {
 font-family: 'JetBrains Mono';
 src: local('JetBrains Mono'), url('/font/JetBrainsMono-Regular.woff2') format('woff2');
}
@font-face {
 font-family: 'Font Awesome 6 Brands';
 src: local('Font Awesome 6 Brands'), url('/lib/fontawesome/fa-brands.woff2') format('woff2');
}
@font-face {
 font-family: 'Font Awesome 6 Free';
 src: local('Font Awesome 6 Free'), url('/lib/fontawesome/fa-regular.woff2') format('woff2');
}</style><style>:root {
  --dark-background: url('https://ak.hypergryph.com/assets/index/images/ak/pc/bk.jpg');
  --light-background: url('/img/bk.jpg');
}</style><meta name="generator" content="Hexo 6.3.0"><style>mjx-container[jax="SVG"] {
  direction: ltr;
}

mjx-container[jax="SVG"] > svg {
  overflow: visible;
}

mjx-container[jax="SVG"][display="true"] {
  display: block;
  text-align: center;
  margin: 1em 0;
}

mjx-container[jax="SVG"][justify="left"] {
  text-align: left;
}

mjx-container[jax="SVG"][justify="right"] {
  text-align: right;
}

g[data-mml-node="merror"] > g {
  fill: red;
  stroke: red;
}

g[data-mml-node="merror"] > rect[data-background] {
  fill: yellow;
  stroke: none;
}

g[data-mml-node="mtable"] > line[data-line] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > rect[data-frame] {
  stroke-width: 70px;
  fill: none;
}

g[data-mml-node="mtable"] > .mjx-dashed {
  stroke-dasharray: 140;
}

g[data-mml-node="mtable"] > .mjx-dotted {
  stroke-linecap: round;
  stroke-dasharray: 0,140;
}

g[data-mml-node="mtable"] > svg {
  overflow: visible;
}

[jax="SVG"] mjx-tool {
  display: inline-block;
  position: relative;
  width: 0;
  height: 0;
}

[jax="SVG"] mjx-tool > mjx-tip {
  position: absolute;
  top: 0;
  left: 0;
}

mjx-tool > mjx-tip {
  display: inline-block;
  padding: .2em;
  border: 1px solid #888;
  font-size: 70%;
  background-color: #F8F8F8;
  color: black;
  box-shadow: 2px 2px 5px #AAAAAA;
}

g[data-mml-node="maction"][data-toggle] {
  cursor: pointer;
}

mjx-status {
  display: block;
  position: fixed;
  left: 1em;
  bottom: 1em;
  min-width: 25%;
  padding: .2em .4em;
  border: 1px solid #888;
  font-size: 90%;
  background-color: #F8F8F8;
  color: black;
}

foreignObject[data-mjx-xml] {
  font-family: initial;
  line-height: normal;
  overflow: visible;
}

.MathJax path {
  stroke-width: 3;
}

mjx-container[display="true"] {
  overflow: auto hidden;
}

mjx-container[display="true"] + br {
  display: none;
}
</style></head><body><div class="loading" style="opacity: 0"><div class="loadingBar left"></div><div class="loadingBar right"></div></div><main><header class="closed"><nav><div class="navBtn hide"><i class="navBtnIcon"><span class="navBtnIconBar"></span><span class="navBtnIconBar"></span><span class="navBtnIconBar"></span></i></div><div class="navItem" id="search-header"><span class="navItemTitle"><input autocomplete="off" autocorrect="off" autocapitalize="none" placeholder="Search" spellcheck="false" maxlength="50" type="text" id="search-input"></span></div><div class="navItem" id="search-holder"></div><div class="search-popup"><div id="search-result"></div></div><ol class="navContent"><li class="navItem"><a class="navBlock" href="/"><span class="navItemTitle">Home</span></a></li><li class="navItem" matchdata="categories,tags"><a class="navBlock" href="/archives/"><span class="navItemTitle">Archives</span></a></li><li class="navItem"><a class="navBlock" href="/about"><span class="navItemTitle">About</span></a></li></ol></nav></header><article><div id="post-bg"><div id="post-title"><h1>Pytorch余弦退火学习率</h1><div id="post-info"><span>First Post: <div class="control"><time datetime="2023-05-27T08:15:33.000Z" id="date"> 2023-05-27</time></div></span><br><span>Last Update: <div class="control"><time datetime="2023-06-16T09:42:37.093Z" id="updated"> 2023-06-16</time></div></span><br><span>Word Count: <div class="control">501</div></span><br><span>Read Time: <div class="control">2 min</div></span></div></div><hr><div id="post-content"><h2 id="CosineAnnealingLR"><a href="#CosineAnnealingLR" class="headerlink" title="CosineAnnealingLR"></a>CosineAnnealingLR</h2><blockquote>
<p>特点是简单好用  [<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html">DOCS</a>] [<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/_modules/torch/optim/lr_scheduler.html#CosineAnnealingLR">CODE</a>] </p>
</blockquote>
<p><code>torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max, eta_min=0, last_epoch=- 1, verbose=False)</code></p>
<p><strong>parameter:</strong></p>
<ul>
<li><strong>optimizer</strong> (<em>Optimizer</em>)– Wrapped optimizer.</li>
<li><strong>T_max</strong> (<em>int</em>) – Maximum number of iterations.</li>
<li><strong>eta_min</strong> (<em>float</em>) – Minimum learning rate. Default: 0.</li>
<li><strong>last_epoch</strong> (<em>int</em>) – The index of last epoch. Default: -1.</li>
<li><strong>verbose</strong> (<em>bool</em>) – If <code>True</code>, prints a message to stdout for each update. Default: <code>False</code>.</li>
</ul>
<p>T_max决定学习率的波动周期，T_max=5时周期为5</p class='item-img' data-src='/2023/05/27/Pytorch%E4%BD%99%E5%BC%A6%E9%80%80%E7%81%AB%E5%AD%A6%E4%B9%A0%E7%8E%87/9ebc4962ead0859d.png'><img src="/2023/05/27/Pytorch%E4%BD%99%E5%BC%A6%E9%80%80%E7%81%AB%E5%AD%A6%E4%B9%A0%E7%8E%87/9ebc4962ead0859d.png" class="" title="Pytorch - 学习率调整策略lr_scheduler - AI备忘录">



<h3 id="CosineAnnealingWarmRestarts"><a href="#CosineAnnealingWarmRestarts" class="headerlink" title="CosineAnnealingWarmRestarts"></a>CosineAnnealingWarmRestarts</h3><blockquote>
<p>[<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingWarmRestarts.html">DOCS</a>] [<a target="_blank" rel="noopener" href="https://pytorch.org/docs/stable/_modules/torch/optim/lr_scheduler.html#CosineAnnealingLR">CODE</a>]</p>
</blockquote>
<p><code>torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0, T_mult=1, eta_min=0, last_epoch=- 1, verbose=False)</code></p>
<p><strong>Parameter:</strong></p>
<ul>
<li><strong>optimizer</strong> (<em>Optimizer</em>) – Wrapped optimizer.</li>
<li><strong>T_0</strong> (<em>int</em>) – Number of iterations for the first restart.</li>
<li><strong>T_mult</strong> (<em>int</em><em>,</em> <em>optional</em>) – A factor increases T_i after a restart. Default: 1.</li>
<li><strong>eta_min</strong> (<em>float</em><em>,</em> <em>optional</em>) – Minimum learning rate. Default: 0.</li>
<li><strong>last_epoch</strong> (<em>int</em><em>,</em> <em>optional</em>) – The index of last epoch. Default: -1.</li>
<li><strong>verbose</strong> (<em>bool</em>) – If <code>True</code>, prints a message to stdout for each update. Default: <code>False</code>.</li>
</ul>
<p>具体地说，</p>
<ul>
<li>T_0：学习率第一次回到初始值的epoch位置</li>
<li>T_mult：控制学习率变化的速度。学习率在 <code>T_0</code>, <code>(1 + T_mult)*T_0</code>, <code>(1 + T_mult + T_mult^2)*T_0</code> …处将回到初始值。</li>
</ul class='item-img' data-src='/2023/05/27/Pytorch%E4%BD%99%E5%BC%A6%E9%80%80%E7%81%AB%E5%AD%A6%E4%B9%A0%E7%8E%87/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM4MjkwNDc1,size_16,color_FFFFFF,t_70.png'><img src="/2023/05/27/Pytorch%E4%BD%99%E5%BC%A6%E9%80%80%E7%81%AB%E5%AD%A6%E4%B9%A0%E7%8E%87/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM4MjkwNDc1,size_16,color_FFFFFF,t_70.png" class="" title="深度学习：学习率规划-余弦退火CosineAnnealing和WarmRestart原理及实现_深度学习退火_Ten_yn的博客-CSDN博客">

<h2 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch.optim.lr_scheduler <span class="hljs-keyword">import</span> CosineAnnealingLR,CosineAnnealingWarmRestarts,StepLR<br><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn<br><span class="hljs-keyword">from</span> torchvision.models <span class="hljs-keyword">import</span> resnet18<br><span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt<br><br>model=resnet18(pretrained=<span class="hljs-literal">False</span>)<br>optimizer = torch.optim.SGD(model.parameters(), lr=<span class="hljs-number">0.1</span>)<br>mode=<span class="hljs-string">'cosineAnnWarm'</span><br><span class="hljs-string">'''</span><br><span class="hljs-string">以T_0=5, T_mult=1为例:</span><br><span class="hljs-string">T_0:学习率第一次回到初始值的epoch位置.</span><br><span class="hljs-string">T_mult:这个控制了学习率回升的速度</span><br><span class="hljs-string">	- 如果T_mult=1,则学习率在T_0,2*T_0,3*T_0,....,i*T_0,....处回到最大值(初始学习率)</span><br><span class="hljs-string">		- 5,10,15,20,25,.......处回到最大值</span><br><span class="hljs-string">	- 如果T_mult&gt;1,则学习率在T_0,(1+T_mult)*T_0,(1+T_mult+T_mult**2)*T_0,.....，(1+T_mult+T_mult**2+...+T_0**i)*T0,处回到最大值</span><br><span class="hljs-string">		- 5,15,35,75,155,.......处回到最大值</span><br><span class="hljs-string">example:</span><br><span class="hljs-string">	T_0=5, T_mult=1</span><br><span class="hljs-string">'''</span><br><span class="hljs-keyword">if</span> mode==<span class="hljs-string">'cosineAnn'</span>:<br>    scheduler = CosineAnnealingLR(optimizer, T_max=<span class="hljs-number">5</span>, eta_min=<span class="hljs-number">0</span>)<br><span class="hljs-keyword">elif</span> mode==<span class="hljs-string">'cosineAnnWarm'</span>:<br>    scheduler = CosineAnnealingWarmRestarts(optimizer,T_0=<span class="hljs-number">5</span>,T_mult=<span class="hljs-number">1</span>)<br>    <br>plt.figure()<br>epochs=<span class="hljs-number">50</span><br>dataloader = Dataloader()<br>cur_lr_list = []<br><span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(epochs):<br>	<span class="hljs-keyword">for</span> (data, label) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(dataloader):<br>		train()<br>		<span class="hljs-built_in">eval</span>()<br>        <span class="hljs-string">'''</span><br><span class="hljs-string">        这里scheduler.step(epoch + batch / iters)的理解如下,如果是一个epoch结束后再.step</span><br><span class="hljs-string">        那么一个epoch内所有batch使用的都是同一个学习率,为了使得不同batch也使用不同的学习率</span><br><span class="hljs-string">        则可以在这里进行.step</span><br><span class="hljs-string">        '''</span><br>        <span class="hljs-comment"># scheduler.step(epoch + batch / iters)</span><br>        optimizer.step()<br>    scheduler.step()<br>    cur_lr=optimizer.param_groups[-<span class="hljs-number">1</span>][<span class="hljs-string">'lr'</span>]<br>    cur_lr_list.append(cur_lr)<br>    <span class="hljs-built_in">print</span>(<span class="hljs-string">'cur_lr:'</span>,cur_lr)<br>x_list = <span class="hljs-built_in">list</span>(<span class="hljs-built_in">range</span>(<span class="hljs-built_in">len</span>(cur_lr_list)))<br>plt.plot(x_list, cur_lr_list)<br>plt.show()<br></code></pre></td></tr></table></figure>

<div id="paginator"></div></div><div id="post-footer"><div id="pages"><div class="footer-link" style="width: 50%;text-align:right;border-right:1px #fe2 solid"><a href="/2023/06/05/%E7%99%BE%E5%BA%A6%E9%A3%9E%E6%A1%A8-PaddleOCR%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83/">← Next 百度飞桨-PaddleOCR模型微调</a></div><div class="footer-link" style="width: 50%;right:1px;border-left:1px #fe2 solid"><a href="/2023/03/11/Linux-Screen-command/">Linux Screen command Prev →</a></div></div></div><div id="comments"><div class="selector"><button class="valine-sel"></button></div><div id="valine"></div></div></div><div class="bottom-btn"><div><a id="to-top" onClick="scrolls.scrolltop();" title="To Top" style="opacity: 0; display: none;">∧</a><a id="to-index" href="#toc-div" title="To Catalog">≡</a><a id="color-mode" onClick="colorMode.change()" title="Change Theme"></a></div></div></article><aside><div id="about"><a href="/" id="logo"><img src="https://ak.hypergryph.com/assets/index/images/ak/pc/faction/1.png" alt="Logo"></a><h1 id="Dr"><a href="/">Tropical Algae</a></h1><div id="description"><p>'海鲜过敏的 热带藻类'</p></div><div id="social-links"><a class="social" target="_blank" rel="noopener" href="https://github.com/LINXIAXING"><i class="fab fa-github" alt="GitHub"></i></a><a class="social" href="mailto:tropicalalgae@gmail.com"><i class="fa fa-envelope" alt="E-Mail"></i></a><a class="social" target="_blank" rel="noopener" href="https://www.twitch.tv/tropicalalgae"><i class="fa-brands fa-twitch" alt="Twitch"></i></a><a class="social" target="_blank" rel="noopener" href="https://space.bilibili.com/38316244"><i class="fa-brands fa-bilibili" alt="Bilibili"></i></a><a class="social" target="_blank" rel="noopener" href="https://discord.com/channels/@me/1017335972351987722"><i class="fab fa-discord" alt="Discord"></i></a><a class="social" target="_blank" rel="noopener" href="https://tropical-algae.notion.site/5de0673f1146409099c0c6dc79d8eadd?v=4573294b03c54d33b98ddb987725ea58&amp;pvs=4"><i class="fa fa-file-text" alt="Notion"></i></a></div></div><div id="aside-block"><div id="toc-div"><h1>Catalog</h1><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#CosineAnnealingLR"><span class="toc-number">1.</span> <span class="toc-text">CosineAnnealingLR</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#CosineAnnealingWarmRestarts"><span class="toc-number">1.1.</span> <span class="toc-text">CosineAnnealingWarmRestarts</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Code"><span class="toc-number">2.</span> <span class="toc-text">Code</span></a></li></ol></div></div><footer><nobr>Published with <a target="_blank" rel="noopener" href="http://hexo.io">Hexo</a></nobr><wbr><nobr> Theme <a target="_blank" rel="noopener" href="https://github.com/Yue-plus/hexo-theme-arknights">Arknights</a></nobr><wbr><nobr>by <a target="_blank" rel="noopener" href="https://github.com/Yue-plus">Yue_plus</a></nobr></footer></aside></main><canvas id="canvas-dust"></canvas><script src="/js/search.js"></script><script src="/js/arknights.js"></script><script src="//unpkg.com/lightgallery@2.7.1/lightgallery.min.js"></script><script src="//unpkg.com/lightgallery@2.7.1/plugins/zoom/lg-zoom.min.js"></script><script src="//unpkg.com/lightgallery@2.7.1/plugins/thumbnail/lg-thumbnail.min.js"></script><script src="/js/pjax.js"></script><script class="pjax-js">reset= () => {new Valine({
 el: '#valine'
 , appId: 'nOR9uzThnw8wi1kNLXHJERAA-gzGzoHsz'
 , appKey: 'T91s4JsMsRkxB6bD4Hp4bayW' , placeholder: 'This comment is sent by Penguin Logistics.'
 , path: window.location.pathname
});code.findCode();
document.querySelector('.lg-container')?.remove()
lightGallery(document.getElementById('post-bg'), {
  plugins: [lgZoom,lgThumbnail],
  selector: '.item-img'})}</script><script>window.addEventListener("load",() => {pjax = new Pjax({
 cacheBust: false,
 selectors: ['title','article','#aside-block','.pjax-js'],
 switches: {'article': Pjax.switches.sideBySide},
 switchesOptions: {
   'article': {
     classNames: {
       remove: "pjax-out",
       add: "pjax-in"
     }
   }
 }
});
document.addEventListener("pjax:complete", reset);reset()})</script></body></html>