[{"title":"ERROR ：gathering device information while adding custom device dev nvidia-uvm“ no such file or dir","url":"/2023/06/12/ERROR%20%EF%BC%9Agathering%20device%20information%20while%20adding%20custom%20device%20dev%20nvidia-uvm%E2%80%9C%20no%20such%20file%20or%20dir/","content":"“/dev/nvidia-uvm“: no such file or dir 问题解决问题描述\n在搭建使用Docker容器搭建PyTorch容器进行深度学习模型训练时，启动容器报错。\n错误信息如下：\n\n\n在查询相关文档后，尝试以下步骤手动解决：\n解决方案首先确保宿主机已安装nvidia驱动。执行以下命令：\nnvidia-smi\n\n确保显卡没问题，尝试手动加载：\ncd /devnvidia-modprobe -u -c=0ls | grep nvidia\n\n\n\nDONE! 问题解决，重启容器即可\n","categories":["ISSUE"],"tags":["Docker","Nvidia"]},{"title":"Docker+FastApi微服务搭建","url":"/2023/06/08/micro-services-based-on-Docker-Nginx-FastApi/","content":"Docker容器下基于Nginx + FastAPI的后端搭建\n本教程仅包含\n\n前言基本思路Docker容器下通过Python fastapi库完成后台开发，Nginx对不同docker下的服务做端口映射，实现微服务开发。不同的镜像负责不同的功能模块，优点是低耦合高延展，且便于迁移（编写脚本实现一键打包与部署，此博客中暂时不做赘述）。\n环境Ubuntu 20.04\nDocker环境搭建\n如果您已准备好容器，请忽略此步\n\n镜像创建与启用准备好镜像后，执行命令启动：\ndocker run -itd --gpus all -e NVIDIA_VISIBLE_DEVICES=all \\-m 128g --cpus=32 --shm-size 64G --name pytorch-container -p 5333:22 \\--ip 172.20.0.1 &lt;image name&gt;:&lt;tag&gt;\n\n–ip 后配置的是容器启动后的IP地址。\n如果忘记可以执行以下命令查看全部容器的地址，或是在容器内部执行ifconfig查看网卡信息：\ndocker inspect -f '{{.Name}} - {{.NetworkSettings.IPAddress }}' $(docker ps -aq)\n\n如果是docker-compose，请使用命令：\ndocker inspect --format='{{.Name}} - {{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' $(docker ps -aq)\n\n完成后请在Xftp和Xshell中链接容器，方便后续操作。\n基于FastApi的后台开发\nFastApi是一个基于Python的高速web框架\n关于更多建议查询官方API，推荐Python版本3.6+\n\n这里我已经写好一份简单的项目。他包含两个接口，通过深度学习模型实现 文字➡语音 和 语音➡文字功能。\n如果要使用，那么你需要拥有基本的CUDA+Pytorch环境，或者你也可以仅做参考。\n详情请访问我的Github仓库。\nNginx端口转发在安装之前，你需要先确保你有安装先决条件\n对于Ubuntu：\napt-get install gccapt-get install libpcre3 libpcre3-devapt-get install zlib1g zlib1g-devapt-get install openssl openssl-dev\n\n对于CentOS：\nyum -y install gcc-c++ yum -y install pcre pcre-develyum -y install zlib zlib-develyum -y install openssl openssl-devel  \n\n\n\nNginx安装访问nginx下载，并将下载好的包上传服务器。或者你可以在服务器上运行以下命令：\nwget http://nginx.org/download/nginx-1.24.0.tar.gz\n\n解压安装包：\ntar -zxvf nginx-1.24.0.tar.gz\n\n\n\n配置转发规则server{  listen 8001;  server_name 127.0.0.1;  location / {    proxy_pass  http://172.20.0.1:8002;    proxy_set_header Host $proxy_host;    proxy_set_header X-Real-IP $remote_addr;    proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;  }}\n\n访问本机8001端口时，将转发到172.20.0.1容器的8002端口\n","categories":["TECHNOLOGY"],"tags":["Docker","Nginx","FastAPI"]},{"title":"百度飞桨-PaddleOCR模型微调","url":"/2023/06/12/%E7%99%BE%E5%BA%A6%E9%A3%9E%E6%A1%A8-PaddleOCR%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83/","content":"PaddleOCR模型微调前言博客将从环境配置到微调记录完整的流程。\n\n最难的步骤是配环境，环境配不好基本就寄了\n\n环境准备Docker环境本次是基于Docker的开发环境，请同学帮忙准备了Paddle镜像。您可以在这里下载需要的相关镜像，或是直接将镜像pull到您的服务器中。\n我的环境配置：\npaddle 2.4.2\ngpu-cuda 11.7\ncudnn 8.4\ntrt 8.4\n准备好镜像后执行命令检查你当前的镜像：\nDocker ps -a\n\n启动镜像：\ndocker run -itd \\-p 5444:22 --name &lt;container name&gt; \\--gpus all -e NVIDIA_VISIBLE_DEVICES=all \\-m 128g --cpus=32 --shm-size=64g \\--restart=always \\--device /dev/nvidiactl:/dev/nvidiactl --device /dev/nvidia-uvm:/dev/nvidia-uvm \\--device /dev/nvidia0:/dev/nvidia0 --device /dev/nvidia1:/dev/nvidia1 --device /dev/nvidia2:/dev/nvidia2 --device /dev/nvidia3:/dev/nvidia3 \\-v /datasets/dataset:/dataset:ro \\&lt;image name&gt;:&lt;tag&gt;\n\nPython库配置模型微调模型测试替换权重识别测试img_path = './test.jpg'ocr = PaddleOCR(use_angle_cls=True, use_gpu=False,lang=\"ch\")  # need to run only once to download and load model into memoryresult = ocr.ocr(img_path, det=True, cls=False, rec=True)# 预测结果包含识别BOX的位置信息，文字以及置信度# [[[x, y]]*4, (word, confidence)]for line in result[0]:\tprint(line)image = Image.open(img_path).convert('RGB')boxes = [line[0] for line in result[0]]txts = [line[1][0] for line in result[0]]scores = [line[1][1] for line in result[0]]im_show = draw_ocr(image, boxes, txts, scores)im_show = Image.fromarray(im_show)im_show.save('./result.jpg')\n\n这是一份基础的测试代码，后续将更新更完整的Acc测试脚本\n","categories":["TECHNOLOGY"],"tags":["Deep Learning","OCR","Paddle"]}]